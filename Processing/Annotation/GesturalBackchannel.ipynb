{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestural Backchannel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to verbal backchannels such as humming, we believe that there are certain gestures that indicate the listener's attention to (and even possibly sentiments towards) the speech. Although these gestures may differ in diffrent cultures, given that almost all of our participants are native to the United States of America, we shall only consider backchannel gestures in the American culture. In particular, we consider a participant to be giving a **gestural backchannel** at a given frame if the participant is either *nodding* or *shaking their head*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not distinguish the types of the gestural backchannel. In particular, we will not record nodding and headshaking separately. This is because we are primarily concerned with how frequent the listener is responding to the speaker in *some* way, rather than *in what way* the listener is responding to the speaker, which may be a helpful feature in tasks such as sentiment analysis. Thus, we will record the frequency of the backchannels without their types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Head Motions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our collected raw data includes a triple $(roll, pitch, yaw)$ of angles to represent each participant's head direction. More specifically, the roll angle is indicative of the motion of tilting one's head towards the shoulders. The pitch angle records lowering and raising one's head as in a nodding motion, and the yaw angle contains information about turning one's head to the left and right. We will utilize these angles to detect head motions of nodding and headshaking.\n",
    "\n",
    "We need first define the head motions of interest in terms of the three head direction angles. Intuitively, in a nodding motion, the pitch angle of the head must follow a fluctuating pattern before eventually stabilizing, and so does the yaw angle in a headshaking motion. Indeed, this is the most widely accepted definition of head motions in relevant works, and we shall adopt the same characterization. \n",
    "\n",
    "Now, to detect head motions in our data, we will adopt a hybrid of programmatic and manual detection of nodding and headshaking. For the programmatic portion, we will employ (with modifications) the straightforward algorithms proposed in this paper. Although this paper is aimed to deal with more primitive forms of data - that is, 2D images of participants' face rather than 3D head angles, we discovered that the approach is simple but captures the essence of the motions rather well and decided that it could be adapted to our data. \n",
    "\n",
    "Note that the algorithms for detecting nodding and headshaking are almost identical except for the head angles used. Below we shall only introduce the headshaking detection algorithm we will use. Replacing all occurrences of \"yaw angle\" in the headshaking detection algorithm with \"pitch angle\" will result in the nodding detection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for headshaking detection we will use, which is based on the same algorithm proposed in this paper and with modifications as we saw fit, is based on the following intuitions: \n",
    "- the start and the end of a headshaking motion must be \"stable\";\n",
    "- the head must reach a leftmost angle and a rightmost angle during a headshaking motion;\n",
    "- the difference between the leftmost angle and the rightmost angle must be somewhat \"significant\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, following the described intuitions, we are ready to introduce the entire programmatic procedure more rigorously. (Footnote: Note that although we have three participants in our data, we will define the following terminologies and procedures for only one participant, and our data will eventually be processed for one participant at the same for a total of three participants.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**: We define the **neighboring frames** of a given frame to be the collection of all frames that are no more than two frames away from the given frame. For example, with $n$ denoting the total number of frames (Footnote: index of frames starts with 0). For $i$ with $i \\in \\mathbb{Z}^+, 2 \\leq i \\leq n -3$, the **neighboring frames** of the $i$-th frame $F_i$ is the set $\\{F_{i-2}, F_{i-1}, F_i, F_{i+1}, F_{i+2}\\}$. For other valid frame indices $i$, some elements in the above set may not exist due to invalid frame indices, in which case we will simply discard them from the set and keep the valid elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**: Given a frame that is neither the starting frame nor the ending frame of the data, the frame is called **stable** if the difference between the maximum and the minimum of the *yaw* angle in the neighboring frames is less than or equal to $5$ (degrees). The frame is called **extreme** if the yaw angle at this frame achieves an extremum in the neighboring frames, and the frame is called **transient** if it is neither stable nor extreme. Finally, the starting and the ending frames are categorized as having none of these three states (since one-sided neighbors alone do not contain sufficient information for a state of motion to be defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**: Given two stable frames, we determine that a headshaking motion has occurred between these two frames if 1) there are at least two extreme frames between the two given stable frames; and 2) for all such extreme frames, each pair of adjacent extreme frames satifsfies that the difference of the yaw angles at these two extreme frames is larger than $5$ (degrees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"significant difference\" threshold (5 degrees) defined above is an empirical value. In fact, after implementing and experimenting with different threshold values, we manually check the original video footage to determine a decent value to characterize the motions and manually fix erroneously detected motions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_date=[\"12-15-2021\",\"01-28-2022\",\"02-11-2022\",\"03-04-2022\",\"03-05-2022\",\"03-11-2022\",\"03-12-2022\"]\n",
    "# all_date=all_date[2:7]\n",
    "# all_sessions=[['1','2','3','4'],['1','2','3','4','5','6'],['2','3'],['1','2','3'],['1','2'],['1','2','3'],['1','2','3']]\n",
    "# all_sessions = all_sessions[2:7]\n",
    "\n",
    "all_date=[\"02-11-2022\",\"03-04-2022\",\"03-05-2022\",\"03-12-2022\"]\n",
    "all_sessions=[['2','3'],['1','2','3'],['1','2'],['1','2','3']]\n",
    "\n",
    "headshaking_threshold = 2\n",
    "nodding_threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02-11-2022', '03-04-2022', '03-05-2022', '03-12-2022']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2', '3'], ['1', '2', '3'], ['1', '2'], ['1', '2', '3']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Assigning Each Frame a State Based on the Neighboring Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle_name should be \"yaw\" for headshaking and \"pitch\" for nodding\n",
    "# p_id is an integer 1, 2, 3 indicating the id of the participant\n",
    "def assign_states(angle_name, p_id, state_name, threshold):\n",
    "    \n",
    "    df = head_angles_for_three[p_id - 1]\n",
    "    angle_col_index = df.columns.get_loc(angle_name) # Angle column index\n",
    "\n",
    "    df[state_name] = \"\" \n",
    "    state_col_index = df.columns.get_loc(state_name)\n",
    "\n",
    "    num_frames = len(df) # Note that the length is the same for three dataframes = the number of frames.\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        # Starting and ending frames have no states\n",
    "        if (i == 0 or i == num_frames - 1):\n",
    "            df.iloc[i, state_col_index] = None \n",
    "        else:\n",
    "            # Frames that are no more than two frames away from the given frame\n",
    "            min_index = max(0, i - 2)\n",
    "            max_index = min(num_frames - 1, i + 2)\n",
    "            neighboring_frames = df.iloc[range(min_index, max_index + 1), :]\n",
    "\n",
    "            # Determine states\n",
    "            max_angle = max(neighboring_frames[angle_name])\n",
    "            min_angle = min(neighboring_frames[angle_name])\n",
    "\n",
    "            if (max_angle - min_angle <= threshold):\n",
    "                df.iloc[i, state_col_index] = \"stable\"\n",
    "            elif (max_angle == df.iloc[i, angle_col_index] or min_angle == df.iloc[i, angle_col_index]):\n",
    "                df.iloc[i, state_col_index] = \"extreme\"\n",
    "            else:\n",
    "                df.iloc[i, state_col_index] = \"transient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement the Algorithmic Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gestural_algorithm(angle_name, p_id, state_name, movement_name, threshold):\n",
    "\n",
    "    df = head_angles_for_three[p_id - 1]\n",
    "\n",
    "    df[movement_name] = 0\n",
    "    movement_col_index = df.columns.get_loc(movement_name)\n",
    "\n",
    "    stable_frames = df[df[state_name] == \"stable\"].reset_index(drop = True)\n",
    "    num_stable_frames = len(stable_frames)\n",
    "    angle_col_index = df.columns.get_loc(angle_name) # Angle column index    \n",
    "\n",
    "    # Given two stable frames...\n",
    "    for index in range(len(stable_frames)):\n",
    "        if (index == num_stable_frames - 1):\n",
    "            break\n",
    "        else:\n",
    "            frame_range_between_stables = range(stable_frames[\"frame\"][index], stable_frames[\"frame\"][index + 1] + 1)\n",
    "\n",
    "            # Count the number of extreme states between the two stable frames\n",
    "            extreme_count = 0\n",
    "            for i in frame_range_between_stables:\n",
    "                if (df[state_name][i] == \"extreme\"):\n",
    "                    extreme_count = extreme_count + 1\n",
    "\n",
    "            if (extreme_count >= 2):\n",
    "                extreme_frames_between = df[df[\"frame\"].isin(frame_range_between_stables)][df[state_name] == \"extreme\"].reset_index(drop = True)\n",
    "\n",
    "                # For each pair of adjacent extreme frames...\n",
    "                headshaking = True\n",
    "                for j in range(len(extreme_frames_between) - 1):\n",
    "                    if (j != len(extreme_frames_between) - 1 and abs(extreme_frames_between.iloc[j, angle_col_index] - extreme_frames_between.iloc[j + 1, angle_col_index]) <= threshold):\n",
    "                        headshaking = False\n",
    "                        break\n",
    "            \n",
    "                if headshaking:\n",
    "                    # Then headshaking has occurred between the two stable frames\n",
    "                    df.iloc[frame_range_between_stables, movement_col_index] = 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-11-2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DreamFall\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-11-2022\n",
      "03-04-2022\n",
      "03-04-2022\n",
      "03-04-2022\n",
      "03-05-2022\n",
      "03-05-2022\n",
      "03-12-2022\n",
      "03-12-2022\n",
      "03-12-2022\n"
     ]
    }
   ],
   "source": [
    "for idd, all_session in enumerate(all_sessions):\n",
    "    date=all_date[idd]\n",
    "    for session in all_session:\n",
    "        # Load Data\n",
    "        print(date)\n",
    "        df_1 = pd.read_table(\"../../Turn taking and keeping/3people/\" + date + \"/Mocap/Separate/Session_\" + session + \"_PC_1_mocap_data.txt\", delimiter = \" \", header = None)\n",
    "        df_2 = pd.read_table(\"../../Turn taking and keeping/3people/\" + date + \"/Mocap/Separate/Session_\" + session + \"_PC_2_mocap_data.txt\", delimiter = \" \", header = None)\n",
    "        df_3 = pd.read_table(\"../../Turn taking and keeping/3people/\" + date + \"/Mocap/Separate/Session_\" + session + \"_PC_3_mocap_data.txt\", delimiter = \" \", header = None)\n",
    "\n",
    "        # Remove the excess space which formed a column\n",
    "        # There should be 1 + 3 * 7 columns in total, with the first column being index of frames, and the 7 groups of three being angles in x, y, and z directions.\n",
    "        # The Head Angles\n",
    "        p1_head_angles = df_1.iloc[:, 30:33]\n",
    "        p2_head_angles = df_2.iloc[:, 30:33]\n",
    "        p3_head_angles = df_3.iloc[:, 30:33]\n",
    "\n",
    "        # Get head angle data from the original df\n",
    "        # Each participant's head angle df\n",
    "        p1_head_angles.columns = [\"roll\", \"pitch\", \"yaw\"]\n",
    "        p2_head_angles.columns = [\"roll\", \"pitch\", \"yaw\"]\n",
    "        p3_head_angles.columns = [\"roll\", \"pitch\", \"yaw\"]\n",
    "        # col_indices = [0, 30, 31, 32]\n",
    "        # head_angles = df.iloc[:, col_indices]\n",
    "        # head_angles.columns = [\"frame\", \"roll\", \"pitch\", \"yaw\"]\n",
    "        # head_angles[\"frame\"] = head_angles[\"frame\"] - 1\n",
    "        p1_head_angles[\"frame\"] = p1_head_angles.index+1\n",
    "        p2_head_angles[\"frame\"] = p2_head_angles.index+1\n",
    "        p3_head_angles[\"frame\"] = p3_head_angles.index+1\n",
    "\n",
    "        # Create a list for them for convenience\n",
    "        head_angles_for_three = [p1_head_angles, p2_head_angles, p3_head_angles]\n",
    "        for i in [1, 2, 3]:\n",
    "            assign_states(\"yaw\", i, \"headshaking_state\", headshaking_threshold)\n",
    "            gestural_algorithm(\"yaw\", i, \"headshaking_state\", \"headshaking\", headshaking_threshold)\n",
    "            \n",
    "        for i in [1, 2, 3]:\n",
    "            assign_states(\"pitch\", i, \"nodding_state\", nodding_threshold)\n",
    "            gestural_algorithm(\"pitch\", i, \"nodding_state\", \"nodding\", nodding_threshold)\n",
    "            \n",
    "        AudioAndGaze = pd.read_csv(\"../Training/AudioAndGaze/\" + date + \"_Session_\" + session + \"_audio_gaze.csv\")\n",
    "\n",
    "        for i in range(3):\n",
    "            headshaking_col_name = \"p\" + str(i + 1) + \"_headshaking\"\n",
    "            nodding_col_name = \"p\" + str(i + 1) + \"_nodding\"\n",
    "\n",
    "            AudioAndGaze[headshaking_col_name] = head_angles_for_three[i][\"headshaking\"]\n",
    "            AudioAndGaze[nodding_col_name] = head_angles_for_three[i][\"nodding\"]\n",
    "            \n",
    "        AudioAndGaze.to_csv(\"../Training/AudioGazeGBack/\" + date + \"_Session_\" + session + \"_audio_gaze_gback.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = \"01-28-2022\"\n",
    "# session = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Data\n",
    "# df_1 = pd.read_table(\"../../Turn taking and keeping/3people/\" + date + \"\\Mocap\\Separate\\Session_\" + session + \"_PC_1_mocap_data.txt\", delimiter = \" \", header = None)\n",
    "# df_2 = pd.read_table(\"../../Turn taking and keeping/3people/\" + date + \"\\Mocap\\Separate\\Session_\" + session + \"_PC_2_mocap_data.txt\", delimiter = \" \", header = None)\n",
    "# df_3 = pd.read_table(\"../../Turn taking and keeping/3people/\" + date + \"\\Mocap\\Separate\\Session_\" + session + \"_PC_3_mocap_data.txt\", delimiter = \" \", header = None)\n",
    "\n",
    "# # Remove the excess space which formed a column\n",
    "# # There should be 1 + 3 * 7 columns in total, with the first column being index of frames, and the 7 groups of three being angles in x, y, and z directions.\n",
    "# # The Head Angles\n",
    "# p1_head_angles = df_1.iloc[:, 30:33]\n",
    "# p2_head_angles = df_2.iloc[:, 30:33]\n",
    "# p3_head_angles = df_3.iloc[:, 30:33]\n",
    "\n",
    "# # Get head angle data from the original df\n",
    "# # Each participant's head angle df\n",
    "# p1_head_angles.columns = [\"roll\", \"pitch\", \"yaw\"]\n",
    "# p2_head_angles.columns = [\"roll\", \"pitch\", \"yaw\"]\n",
    "# p3_head_angles.columns = [\"roll\", \"pitch\", \"yaw\"]\n",
    "# # col_indices = [0, 30, 31, 32]\n",
    "# # head_angles = df.iloc[:, col_indices]\n",
    "# # head_angles.columns = [\"frame\", \"roll\", \"pitch\", \"yaw\"]\n",
    "# # head_angles[\"frame\"] = head_angles[\"frame\"] - 1\n",
    "# p1_head_angles[\"frame\"] = p1_head_angles.index+1\n",
    "# p2_head_angles[\"frame\"] = p2_head_angles.index+1\n",
    "# p3_head_angles[\"frame\"] = p3_head_angles.index+1\n",
    "\n",
    "# # Create a list for them for convenience\n",
    "# head_angles_for_three = [p1_head_angles, p2_head_angles, p3_head_angles]\n",
    "# AudioAndGaze = pd.read_csv(\"../Training/AudioAndGaze/\" + date + \"_Session_\" + session + \"_audio_gaze.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Headshaking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1, 2, 3]:\n",
    "#     assign_states(\"yaw\", i, \"headshaking_state\", headshaking_threshold)\n",
    "#     gestural_algorithm(\"yaw\", i, \"headshaking_state\", \"headshaking\", headshaking_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Nodding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in [1, 2, 3]:\n",
    "#     assign_states(\"pitch\", i, \"nodding_state\", nodding_threshold)\n",
    "#     gestural_algorithm(\"pitch\", i, \"nodding_state\", \"nodding\", nodding_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# p1_head_angles.index[p1_head_angles['nodding']==1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to AudioAndGaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AudioAndGaze = pd.read_csv(\"../Training/AudioAndGaze/\" + date + \"_Session_\" + session + \"_audio_gaze.csv\")\n",
    "\n",
    "# for i in range(3):\n",
    "#     headshaking_col_name = \"p\" + str(i + 1) + \"_headshaking\"\n",
    "#     nodding_col_name = \"p\" + str(i + 1) + \"_nodding\"\n",
    "\n",
    "#     AudioAndGaze[headshaking_col_name] = head_angles_for_three[i][\"headshaking\"]\n",
    "#     AudioAndGaze[nodding_col_name] = head_angles_for_three[i][\"nodding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioAndGaze.to_csv(\"../Training/AudioGazeGBack/\" + date + \"_Session_\" + session + \"_audio_gaze_gback.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c0b179fde42f9ad324daf73aa4e1445a98433cd53eebb5e6d9443e56c91ba16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
