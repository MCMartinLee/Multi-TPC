{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d4e75f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import srt\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e257674",
   "metadata": {},
   "source": [
    "1. Set the date and session directory\n",
    "2. Choose the person to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "29187da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_date=[\"12-15-2021\",\"01-28-2022\",\"02-11-2022\",\"03-04-2022\",\"03-05-2022\",\"03-11-2022\",\"03-12-2022\"]\n",
    "date=all_date[5]\n",
    "total_session=['1','2','3']\n",
    "all_pc=['1','2','3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "2fa44959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03-11-2022'"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "56132db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in total_session:\n",
    "    for pc in all_pc:\n",
    "        sentences=[]\n",
    "        data=\"../3people/\"+date+\"/subtitles/Session_\"+session+\"_PC_\"+pc+\"_Captions.\"\n",
    "        clean_data=\"../3people/\"+date+\"/Caption/Session_\"+session+\"_PC_\"+pc\n",
    "        f = open(data+\"vtt\", \"r\",encoding=\"utf-8\")\n",
    "        cc= open(data+\"srt\",\"r\",encoding=\"utf-8\")\n",
    "        subs = list(srt.parse(cc))\n",
    "        for line in f:\n",
    "            sentences.append(line.replace(\"\\n\", \"\"))\n",
    "        zero = datetime(1999, 1, 1,0,0,0,000000)\n",
    "        all_sentences = pd.DataFrame(columns=['start', 'end', 'Sentence'])\n",
    "        all_words_by_sentence = pd.DataFrame(columns=['id', 'start', 'end', 'word'])\n",
    "        empty_word = {'id':'','start':'','end':'','word':''}\n",
    "        sub_index=0\n",
    "        for i in range(len(sentences)):\n",
    "            start=zero+subs[sub_index].start\n",
    "            start=start.strftime('%H:%M:%S.%f')[:-3]\n",
    "            end=zero+subs[sub_index].end\n",
    "            end=end.strftime('%H:%M:%S.%f')[:-3]\n",
    "            if start in sentences[i][:12]:\n",
    "                ID=0\n",
    "                if \"<c>\" in sentences[i+2]:\n",
    "\n",
    "\n",
    "                    a= re.split('<c> |</c>|<|>',sentences[i+2])\n",
    "                    while(\"\" in a):\n",
    "                        a.remove(\"\")\n",
    "                    sentence=None\n",
    "                    element = datetime.strptime(a[len(a)-2], '%H:%M:%S.%f')\n",
    "                    result_1 = element + timedelta(milliseconds=200)\n",
    "                    for k in range(len(a)):\n",
    "                        if k==len(a)-1:\n",
    "                            sentence= sentence +\" \" + a[k]\n",
    "                            df1={'id':str(ID),'start':a[k-1],'end':result_1.strftime('%H:%M:%S.%f')[:-3],'word':a[k]}\n",
    "                            ID=ID+1\n",
    "                            all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "                        elif k==0:\n",
    "                            sentence= a[k] \n",
    "                            df1={'id':str(ID),'start':start,'end':a[k+1],'word':a[k]}\n",
    "                            ID=ID+1\n",
    "                            all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "                        elif k % 2 == 0:\n",
    "                            sentence= sentence +\" \" + a[k]\n",
    "                            df1={'id':str(ID),'start':a[k-1],'end':a[k+1],'word':a[k]}\n",
    "                            ID=ID+1\n",
    "                            all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "\n",
    "                    df2 = {'start': start, 'end': result_1.strftime('%H:%M:%S.%f')[:-3], 'Sentence': sentence}\n",
    "\n",
    "                    all_sentences = all_sentences.append(df2, ignore_index = True)\n",
    "\n",
    "                    all_words_by_sentence = all_words_by_sentence.append(empty_word, ignore_index = True)\n",
    "\n",
    "                else:\n",
    "                    df1 = df1={'id':str(ID),'start':start,'end':end,'word':sentences[i+2]}\n",
    "                    df2 = {'start':start,'end':end, 'Sentence': sentences[i+2]}\n",
    "                    all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "                    all_words_by_sentence = all_words_by_sentence.append(empty_word, ignore_index = True)\n",
    "                    all_sentences = all_sentences.append(df2, ignore_index = True)\n",
    "\n",
    "                sub_index= sub_index+1\n",
    "                if sub_index == len(subs):\n",
    "                    break\n",
    "        f.close()  \n",
    "        cc.close()\n",
    "        #Define threshold of gap\n",
    "        threshold = timedelta(milliseconds=200)\n",
    "        for i in range(len(all_sentences)-1, 0, -1):\n",
    "            if datetime.strptime(all_sentences['start'][i], '%H:%M:%S.%f') - datetime.strptime(all_sentences['end'][i-1], '%H:%M:%S.%f') < threshold:\n",
    "                all_sentences['end'][i-1]=all_sentences['end'][i]\n",
    "                all_sentences['Sentence'][i-1]= all_sentences['Sentence'][i-1] + \" \" + all_sentences['Sentence'][i]\n",
    "                all_sentences = all_sentences.drop(i)\n",
    "        all_sentences = all_sentences.reset_index()\n",
    "        all_sentences=all_sentences.drop(columns=['index'])\n",
    "        for i in range(len(all_words_by_sentence)-2, -1, -1):\n",
    "            if all_words_by_sentence['word'][i]==\"\":\n",
    "                if datetime.strptime(all_words_by_sentence['start'][i+1], '%H:%M:%S.%f') - datetime.strptime(all_words_by_sentence['end'][i-1], '%H:%M:%S.%f') < threshold:\n",
    "                    all_words_by_sentence['end'][i-1] = all_words_by_sentence['start'][i+1] \n",
    "                    all_words_by_sentence = all_words_by_sentence.drop(i)\n",
    "        all_words_by_sentence = all_words_by_sentence.reset_index()\n",
    "        all_words_by_sentence=all_words_by_sentence.drop(columns=['index'])\n",
    "        present= len(all_words_by_sentence)-1\n",
    "        previous= present\n",
    "        for i in range(len(all_words_by_sentence)-2, -1, -1):\n",
    "            if all_words_by_sentence['word'][i]==\"\":\n",
    "                present= i\n",
    "\n",
    "                for k in range(1,previous-present):\n",
    "                    all_words_by_sentence['id'][i+k]=k-1\n",
    "                previous= i \n",
    "        all_sentences['SID']='0'\n",
    "        all_words_by_sentence['SID']='0'\n",
    "\n",
    "        for i in range(len(all_sentences)):\n",
    "            all_sentences['SID'][i]=date+\"_Session_\"+session+\"_PC_\"+pc+\"_\"+str(i+1)\n",
    "\n",
    "        k=1\n",
    "        for i in range(len(all_words_by_sentence)):\n",
    "            if all_words_by_sentence['word'][i]==\"\":\n",
    "                k=k+1\n",
    "                all_words_by_sentence['SID'][i]=''\n",
    "            else:\n",
    "                all_words_by_sentence['SID'][i]=date+\"_Session_\"+session+\"_PC_\"+pc+\"_\"+str(k)\n",
    "\n",
    "        all_sentences.to_csv(clean_data+'_sentence.csv',index=False)   \n",
    "        all_words_by_sentence.to_csv(clean_data+'_words.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "53bc71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date=\"03-12-2022\"\n",
    "# session=\"3\"\n",
    "# pc=\"3\"\n",
    "# sentences=[]\n",
    "# data=\"../3people/\"+date+\"/subtitles/Session_\"+session+\"_PC_\"+pc+\"_Captions.\"\n",
    "# clean_data=\"../3people/\"+date+\"/Caption/Session_\"+session+\"_PC_\"+pc\n",
    "# f = open(data+\"vtt\", \"r\",encoding=\"utf-8\")\n",
    "# cc= open(data+\"srt\",\"r\",encoding=\"utf-8\")\n",
    "# subs = list(srt.parse(cc))\n",
    "# for line in f:\n",
    "#     sentences.append(line.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cbe0f",
   "metadata": {},
   "source": [
    "Set a zero time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "46f7026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero = datetime(1999, 1, 1,0,0,0,000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "7f64d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentences = pd.DataFrame(columns=['start', 'end', 'Sentence'])\n",
    "# all_words_by_sentence = pd.DataFrame(columns=['id', 'start', 'end', 'word'])\n",
    "# empty_word = {'id':'','start':'','end':'','word':''}\n",
    "# sub_index=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "00b9c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(sentences)):\n",
    "#     start=zero+subs[sub_index].start\n",
    "#     start=start.strftime('%H:%M:%S.%f')[:-3]\n",
    "#     end=zero+subs[sub_index].end\n",
    "#     end=end.strftime('%H:%M:%S.%f')[:-3]\n",
    "#     if start in sentences[i][:12]:\n",
    "#         ID=0\n",
    "#         if \"<c>\" in sentences[i+2]:\n",
    "        \n",
    "            \n",
    "#             a= re.split('<c> |</c>|<|>',sentences[i+2])\n",
    "#             while(\"\" in a):\n",
    "#                 a.remove(\"\")\n",
    "#             sentence=None\n",
    "#             element = datetime.strptime(a[len(a)-2], '%H:%M:%S.%f')\n",
    "#             result_1 = element + timedelta(milliseconds=200)\n",
    "#             for k in range(len(a)):\n",
    "#                 if k==len(a)-1:\n",
    "#                     sentence= sentence +\" \" + a[k]\n",
    "#                     df1={'id':str(ID),'start':a[k-1],'end':result_1.strftime('%H:%M:%S.%f')[:-3],'word':a[k]}\n",
    "#                     ID=ID+1\n",
    "#                     all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "#                 elif k==0:\n",
    "#                     sentence= a[k] \n",
    "#                     df1={'id':str(ID),'start':start,'end':a[k+1],'word':a[k]}\n",
    "#                     ID=ID+1\n",
    "#                     all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "#                 elif k % 2 == 0:\n",
    "#                     sentence= sentence +\" \" + a[k]\n",
    "#                     df1={'id':str(ID),'start':a[k-1],'end':a[k+1],'word':a[k]}\n",
    "#                     ID=ID+1\n",
    "#                     all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "           \n",
    "#             df2 = {'start': start, 'end': result_1.strftime('%H:%M:%S.%f')[:-3], 'Sentence': sentence}\n",
    "\n",
    "#             all_sentences = all_sentences.append(df2, ignore_index = True)\n",
    "    \n",
    "#             all_words_by_sentence = all_words_by_sentence.append(empty_word, ignore_index = True)\n",
    "            \n",
    "#         else:\n",
    "#             df1 = df1={'id':str(ID),'start':start,'end':end,'word':sentences[i+2]}\n",
    "#             df2 = {'start':start,'end':end, 'Sentence': sentences[i+2]}\n",
    "#             all_words_by_sentence = all_words_by_sentence.append(df1, ignore_index = True)\n",
    "#             all_words_by_sentence = all_words_by_sentence.append(empty_word, ignore_index = True)\n",
    "#             all_sentences = all_sentences.append(df2, ignore_index = True)\n",
    "            \n",
    "#         sub_index= sub_index+1\n",
    "#         if sub_index == len(subs):\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e89a6f",
   "metadata": {},
   "source": [
    "Close open files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "8f0e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.close()  \n",
    "# cc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc62d6",
   "metadata": {},
   "source": [
    "# Combine two sentences if the gap is too short, shorter than 200ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "14c424e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define threshold of gap\n",
    "# threshold = timedelta(milliseconds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "a9ae2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(all_sentences)-1, 0, -1):\n",
    "#     if datetime.strptime(all_sentences['start'][i], '%H:%M:%S.%f') - datetime.strptime(all_sentences['end'][i-1], '%H:%M:%S.%f') < threshold:\n",
    "#         all_sentences['end'][i-1]=all_sentences['end'][i]\n",
    "#         all_sentences['Sentence'][i-1]= all_sentences['Sentence'][i-1] + \" \" + all_sentences['Sentence'][i]\n",
    "#         all_sentences = all_sentences.drop(i)\n",
    "# all_sentences = all_sentences.reset_index()\n",
    "# all_sentences=all_sentences.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "9cfd6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(all_words_by_sentence)-2, -1, -1):\n",
    "#     if all_words_by_sentence['word'][i]==\"\":\n",
    "#         if datetime.strptime(all_words_by_sentence['start'][i+1], '%H:%M:%S.%f') - datetime.strptime(all_words_by_sentence['end'][i-1], '%H:%M:%S.%f') < threshold:\n",
    "#             all_words_by_sentence['end'][i-1] = all_words_by_sentence['start'][i+1] \n",
    "#             all_words_by_sentence = all_words_by_sentence.drop(i)\n",
    "# all_words_by_sentence = all_words_by_sentence.reset_index()\n",
    "# all_words_by_sentence=all_words_by_sentence.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "d569d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# present= len(all_words_by_sentence)-1\n",
    "# previous= present\n",
    "# for i in range(len(all_words_by_sentence)-2, -1, -1):\n",
    "#     if all_words_by_sentence['word'][i]==\"\":\n",
    "#         present= i\n",
    "        \n",
    "#         for k in range(1,previous-present):\n",
    "#             all_words_by_sentence['id'][i+k]=k-1\n",
    "#         previous= i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "c26e1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentences['SID']='0'\n",
    "# all_words_by_sentence['SID']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "284a1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(all_sentences)):\n",
    "#     all_sentences['SID'][i]=date+\"_Session_\"+session+\"_PC_\"+pc+\"_\"+str(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "9f7cd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=1\n",
    "# for i in range(len(all_words_by_sentence)):\n",
    "#     if all_words_by_sentence['word'][i]==\"\":\n",
    "#         k=k+1\n",
    "#         all_words_by_sentence['SID'][i]=''\n",
    "#     else:\n",
    "#         all_words_by_sentence['SID'][i]=date+\"_Session_\"+session+\"_PC_\"+pc+\"_\"+str(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a5150",
   "metadata": {},
   "source": [
    "Export captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "20397868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sentences.to_csv(clean_data+'_sentence.csv',index=False)   \n",
    "# all_words_by_sentence.to_csv(clean_data+'_words.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
